Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) is a simple optimization technique which forms the building block in training neural networks.
But we all have used this in our daily life in one way or the other. I'll give one example. Remember the days when we
used TV antennas. Whenever we see grainy images, we had to adjust antenna's alignment.
First, we change the antenna's position in different directions and find which direction gave better results. Then, we make further adjustments in the same direction until we get fine image.
This is exactly gradient descent does. We make some adjustments to variables in such a way to get maximum performance out of the system.
